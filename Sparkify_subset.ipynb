{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Sample dataset notebook\n",
    "This notebook contains steps of exploration, processing and modeling with a tiny subset (128MB) of the full dataset available (12GB). Full dataset is treated separately in the notebook on AWS platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimestamp coefficient\n",
    "TS_COEF = 1000*60*60*24\n",
    "\n",
    "# today date\n",
    "TODAY = str(datetime.today().date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this notebook we load the mini-dataset from locally stored file `mini_sparkify_event_data.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Since we are looking at a small subset, it's quite convenient to perform EDA using pandas.\n",
    "Our analysis consists of 3 steps:\n",
    "* Explore Data\n",
    "* Define Churn\n",
    "* Explore churned vs stayed users\n",
    "\n",
    "#### Explore Data\n",
    "I used [sweetviz](https://pypi.org/project/sweetviz/) package to visualize data and make first observations. At this stage we identify the structure of each column, check the nulls and ranges/lists of column values.\n",
    "\n",
    "#### Define Churn\n",
    "I create a column `churn` to use as the label for your model. I used the `Cancellation Confirmation` events to define the churn, which happen for both paid and free users.\n",
    "\n",
    "#### Explore churned vs stayed users\n",
    "Once we've defined churn, we run exploratory data analysis by comparing users who stayed vs users who churned. This is important for the next stage of feature engineering. Looking at major differences, we define the logic for user-level features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA observations\n",
    "I convert Spark dataframe into pandas dataframe to run EDA with more flexibility. Using `sweetviz` I look at the major properties of each column. \n",
    "\n",
    "Here are the **first observations**:\n",
    "1. There are 225 registered users in the dataset and 2354 sessions during 63 days. 97% of records cover the events for these users and only 3% include the data about the guests.\n",
    "2. For guest users (`auth='Guest'`) we don't have neiver songs data or user demographics data, nor *userId*. Their page visits are limited to: Home, Help, Register, About, Submit Registration, Error. We exclude guest users from model dataset.\n",
    "3. There are 3% of records with `auth='Logged Out'`, which include Home, Login, About, Help and Error events. There is no *userId* data for these events, so we exclude them from modelling dataset.\n",
    "3. 80% of records describe NextSong event and include artist and song data. 20% of events cover all over possible actions.\n",
    "4. We have 52 cancellation events, which are described by `auth='Cancelled'` and `page='Cancellation Confirmation'`. There are 52 unique userId, who cancelled subscription. So this event is unique per user.]\n",
    "\n",
    "We **define Churn** as a fact of cancellation of subscription from existing user. The fact of cancellation is translated through 2 columns: `auth='Cancelled'` and `page='Cancellation Confirmation'`, which are uniquely defined, so we can use any of 2 to define the target. Let's use `page='Cancellation Confirmation'` as our target. \n",
    "\n",
    "Before moving to the Feature engineering step, let's **compare** behaviour of **churned users VS stayed users**. Here are some observations:\n",
    "1. Among those who churn there are more males (57% M / 43% F), and vice versa, there are more women among those users, who stay subscrbed (42% M / 58% F).\n",
    "2. Churned users usually have smaller number of items in session (churn median = 66 VS other median = 71).\n",
    "3. Among churned users there are more free users (28% in churn VS 19% in other)\n",
    "4. From page events statistics we see that among churned there are less *Thumbs Up*, more *Thumbs Down*, almost two times higher frequency of *Roll Advert*.\n",
    "5. Churned users have smaller lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>277.89016</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Iron Frenzy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>Long</td>\n",
       "      <td>236.09424</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352180000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist       auth firstName gender  itemInSession lastName  \\\n",
       "0    Martha Tilston  Logged In     Colin      M             50  Freeman   \n",
       "1  Five Iron Frenzy  Logged In     Micah      M             79     Long   \n",
       "\n",
       "      length level                        location method      page  \\\n",
       "0  277.89016  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "1  236.09424  free  Boston-Cambridge-Newton, MA-NH    PUT  NextSong   \n",
       "\n",
       "   registration  sessionId       song  status             ts  \\\n",
       "0  1.538173e+12         29  Rockpools     200  1538352117000   \n",
       "1  1.538332e+12          8     Canada     200  1538352180000   \n",
       "\n",
       "                                           userAgent userId  \n",
       "0  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "1  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_data = df.toPandas()\n",
    "pandas_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286500, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of records and columns in the subset\n",
    "pandas_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date is 2018-10-01\n",
      "Last date is 2018-12-03\n",
      "Total number of days: 63\n"
     ]
    }
   ],
   "source": [
    "# explore time range of data provided\n",
    "print('Earliest date is', pd.to_datetime(pandas_data['ts'], unit='ms').dt.date.min())\n",
    "print('Last date is', pd.to_datetime(pandas_data['ts'], unit='ms').dt.date.max())\n",
    "print('Total number of days:', (pandas_data['ts'].max() - pandas_data['ts'].min())//(TS_COEF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1274eb628d443e39a593a2fd4c67b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "                                             |          | [  0%]   00:00 -> (? left)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report ./EDA_reports/sample_data_overview.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "# generate sweetviz report\n",
    "analysis = sv.analyze([pandas_data, 'sample_data'])\n",
    "analysis.show_html('./EDA_reports/sample_data_overview.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97 entries, 97633 to 199445\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   artist         0 non-null      object \n",
      " 1   auth           97 non-null     object \n",
      " 2   firstName      0 non-null      object \n",
      " 3   gender         0 non-null      object \n",
      " 4   itemInSession  97 non-null     int64  \n",
      " 5   lastName       0 non-null      object \n",
      " 6   length         0 non-null      float64\n",
      " 7   level          97 non-null     object \n",
      " 8   location       0 non-null      object \n",
      " 9   method         97 non-null     object \n",
      " 10  page           97 non-null     object \n",
      " 11  registration   0 non-null      float64\n",
      " 12  sessionId      97 non-null     int64  \n",
      " 13  song           0 non-null      object \n",
      " 14  status         97 non-null     int64  \n",
      " 15  ts             97 non-null     int64  \n",
      " 16  userAgent      0 non-null      object \n",
      " 17  userId         97 non-null     object \n",
      "dtypes: float64(2), int64(4), object(12)\n",
      "memory usage: 14.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# explore data structure for guest visitors\n",
    "pandas_data[pandas_data['auth']=='Guest'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Home                   36\n",
       "Help                   23\n",
       "Register               18\n",
       "About                  14\n",
       "Submit Registration     5\n",
       "Error                   1\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check what actions are available for guest users\n",
    "pandas_data[pandas_data['auth']=='Guest']['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Home                         14457\n",
       "Thumbs Up                    12551\n",
       "Add to Playlist               6526\n",
       "Add Friend                    4277\n",
       "Roll Advert                   3933\n",
       "Login                         3241\n",
       "Logout                        3226\n",
       "Thumbs Down                   2546\n",
       "Downgrade                     2055\n",
       "Help                          1726\n",
       "Settings                      1514\n",
       "About                          924\n",
       "Upgrade                        499\n",
       "Save Settings                  310\n",
       "Error                          258\n",
       "Submit Upgrade                 159\n",
       "Submit Downgrade                63\n",
       "Cancellation Confirmation       52\n",
       "Cancel                          52\n",
       "Register                        18\n",
       "Submit Registration              5\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the events associated with empty song data\n",
    "pandas_data[pandas_data['song'].isnull()]['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSong    228108\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the events associated with full song data\n",
    "pandas_data[~pandas_data['song'].isnull()]['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cancellation Confirmation    52\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check page events associated with Cancelled status\n",
    "pandas_data[pandas_data['auth']=='Cancelled']['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cancelled</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guest</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logged In</th>\n",
       "      <td>225</td>\n",
       "      <td>278102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logged Out</th>\n",
       "      <td>1</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userId      ts\n",
       "auth                      \n",
       "Cancelled       52      52\n",
       "Guest            1      97\n",
       "Logged In      225  278102\n",
       "Logged Out       1    8249"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique user and total count per different values of auth status\n",
    "pandas_data.groupby('auth').agg({'userId': pd.Series.nunique,\n",
    "                                 'ts': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4296b2dc228e4145998bf7628d9e1db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "                                             |          | [  0%]   00:00 -> (? left)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report ./EDA_reports/Churn vs stayed.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "# compare 2 subsets: churned users VS stayed users\n",
    "known_users_df = pandas_data[pandas_data['userId']!=''].copy()\n",
    "\n",
    "# add lifetime column\n",
    "known_users_df['max_ts'] = known_users_df.groupby('userId')['ts'].transform('max')\n",
    "known_users_df['lifetime'] = (known_users_df['max_ts']-known_users_df['registration'])/TS_COEF\n",
    "\n",
    "# list of churned users\n",
    "churned_uid_list = pandas_data[pandas_data['page']=='Cancellation Confirmation']['userId'].to_list()\n",
    "\n",
    "report = sv.compare_intra(known_users_df, known_users_df['userId'].isin(churned_uid_list), [\"Churn\", \"Stayed\"])\n",
    "report.show_html('./EDA_reports/Churn vs stayed.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date of cancellation is 2018-10-01\n",
      "Last date of cancellation is 2018-11-29\n",
      "Total number of days between first and last: 58\n"
     ]
    }
   ],
   "source": [
    "# explore time range of cancellations\n",
    "cancel_data = pandas_data[pandas_data['page']=='Cancellation Confirmation']\n",
    "\n",
    "print('Earliest date of cancellation is', pd.to_datetime(cancel_data['ts'], unit='ms').dt.date.min())\n",
    "print('Last date of cancellation is', pd.to_datetime(cancel_data['ts'], unit='ms').dt.date.max())\n",
    "print('Total number of days between first and last:', (cancel_data['ts'].max() - cancel_data['ts'].min())//(TS_COEF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the modelling dataset\n",
    "1. Exclude records with empty *userId*.\n",
    "2. Add label: 1 = Churn, 0 = Not churn. Condition: `page='Cancellation Confirmation'`\n",
    "3. Remove records of `page='Cancellation Confirmation'`.\n",
    "4. Sort dataframe by `userId` and `ts`\n",
    "5. Aggregate features at user level:\n",
    "    * create list of songs\n",
    "    * create list of artists\n",
    "    * list of page events (Cancellation Confirmation preliminary filtered out to remove the leak)\n",
    "    * session frequency\n",
    "    * average number of songs per session\n",
    "    * binary feature: Male gender = 1/0\n",
    "    * binary feature: paid acoount = 1/0\n",
    "    * lifetime (days): time difference between last activity and registration date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Aggregate user-level properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(df.userId).orderBy(df.ts)\n",
    "w_uid = Window.partitionBy(df.userId)\n",
    "\n",
    "preprocessed_df = (df\n",
    "                   .filter(F.col('userId')!='') #filter out guests\n",
    "                   .withColumn('cancelled', (F.col('page')=='Cancellation Confirmation').cast(IntegerType())) \n",
    "                   .withColumn('churn', F.max('cancelled').over(w_uid)) # define churn label\n",
    "                   .withColumn('current_level', F.last('level').over(w)) # sort levels of subscription by date\n",
    "                   .withColumn('last_userAgent', F.last('userAgent').over(w)) # sort agents by date\n",
    "                   .filter(F.col('page')!='Cancellation Confirmation') #remove page event from dataset\n",
    "                   .groupby('userId') # aggregate features at user level\n",
    "                   .agg(F.collect_list('artist').alias('artist_list'), # combine into list all artist\n",
    "                        F.collect_list('song').alias('song_list'), # combine into list all songs\n",
    "                        F.collect_list('page').alias('page_list'), # combine into list all page events\n",
    "                        F.countDistinct('sessionId').alias('session_count'), # calculate total number of sessions\n",
    "                        F.count('song').alias('song_count'), # calculate total number of songs\n",
    "                        F.first('gender').alias('gender'), # gender data\n",
    "                        F.last('current_level').alias('current_level'), # take last level value\n",
    "                        F.max('churn').alias('churn'), \n",
    "                        F.min('ts').alias('min_ts'), # start timestamp \n",
    "                        F.max('ts').alias('max_ts'), # end timestamp\n",
    "                        F.last('last_userAgent').alias('last_userAgent'), # recent agent\n",
    "                        F.min('registration').alias('registration') # registration date\n",
    "                       )\n",
    "                   # frequency of sessions\n",
    "                   .withColumn('session_freq', F.col('session_count')/((F.col('max_ts')-F.col('min_ts'))/TS_COEF))\n",
    "                   # avg number of songs per session\n",
    "                   .withColumn('song_per_session', F.col('song_count')/F.col('session_count'))\n",
    "                   # binary feature: Male = 1/0\n",
    "                   .withColumn('gender_Male', (F.col('gender')=='M').cast(IntegerType()))\n",
    "                   # binary feature: paid = 1/0\n",
    "                   .withColumn('is_paid', (F.col('current_level')=='paid').cast(IntegerType()))\n",
    "                   # lifetime\n",
    "                   .withColumn('lifetime', (F.col('max_ts')-F.col('registration'))/TS_COEF)\n",
    "                   # extract device/OS pointers from agent\n",
    "                   .withColumn('agent_Windows', F.col('last_userAgent').contains('Windows').cast(IntegerType()))\n",
    "                   .withColumn('agent_Mac', F.col('last_userAgent').contains('Mac').cast(IntegerType()))\n",
    "                   .withColumn('agent_iPhone', F.col('last_userAgent').contains('iPhone').cast(IntegerType()))\n",
    "                   .withColumn('agent_iPad', F.col('last_userAgent').contains('iPad').cast(IntegerType()))\n",
    "                   .withColumn('agent_Linux', F.col('last_userAgent').contains('Linix').cast(IntegerType()))\n",
    "                  ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  count\n",
       "0      1     52\n",
       "1      0    173"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.groupby('churn').count().toPandas()\n",
    "\n",
    "# churn <--> 23%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Prepare transformers to collect feature vector\n",
    "\n",
    "Used features:\n",
    "* Apply TF-IDF to artist list, song list and page list. We limit vocabSize to 100 elements\n",
    "* Beside TF-IDF generated features keep session frequency, avg number of songs per session, lifetime, gender, paid, agent based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_transformer(list_name: str,\n",
    "                       vocabSize: int=100):\n",
    "    \"\"\"\n",
    "    Combines TF and IDF pyspark transformers\n",
    "    ------------\n",
    "    \n",
    "    Args:\n",
    "        list_name (string) : prefix of the feature with work list in the format\n",
    "            prefix_list\n",
    "        vocabSize (int)    : number of top-output words to keep\n",
    "    \n",
    "    Returns:\n",
    "        tf transformer, idf transformer\n",
    "    \"\"\"\n",
    "    tf = CountVectorizer(inputCol=f\"{list_name}_list\", outputCol=f\"TF_{list_name}\", vocabSize=vocabSize)\n",
    "    tf_idf = IDF(inputCol=f\"TF_{list_name}\", outputCol=f\"TFIDF_{list_name}\")\n",
    "    return tf, tf_idf\n",
    "\n",
    "\n",
    "artist_tf, artist_tf_idf = tf_idf_transformer('artist')\n",
    "song_tf, song_tf_idf = tf_idf_transformer('song')\n",
    "page_tf, page_tf_idf = tf_idf_transformer('page')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"TFIDF_artist\", \"TFIDF_song\", \"TFIDF_page\",\n",
    "                                       \"session_freq\", \"song_per_session\", \n",
    "                                       \"lifetime\", \"gender_Male\", \n",
    "                                       \"is_paid\", \"agent_Windows\",\n",
    "                                       \"agent_Mac\", \"agent_iPhone\", \"agent_iPad\", \n",
    "                                       \"agent_Linux\"], \n",
    "                            outputCol=\"features\", \n",
    "                            handleInvalid=\"skip\")\n",
    "\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[artist_tf, artist_tf_idf, \n",
    "                                   song_tf, song_tf_idf,\n",
    "                                   page_tf, page_tf_idf,\n",
    "                                   assembler\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = feature_pipeline.fit(preprocessed_df)\n",
    "test.transform(preprocessed_df).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "We split the full dataset into train (70%) and test (30%). During cross-validation process train data is additionally split into train and validation subsets. Test data is used only to check the model (nexer seen during training).\n",
    "\n",
    "We try 2 models:\n",
    "* Random Forest Classifier\n",
    "* Gradient Boosted Tree Classifier\n",
    "\n",
    "Note: since we use tree-based models, we don't don't need to scale numerical features.\n",
    "Our problem is imbalanced: 23% of positive cases (churn) and 67% of negative (stayed). Thus, we use F1-score to tune hyperparameters and check final quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = preprocessed_df.randomSplit([0.7, 0.3], seed=10)\n",
    "\n",
    "# cache dataframes\n",
    "train_data = train_data.cache()\n",
    "test_data = test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_the_model(test_data, model, metric_name='accuracy'):\n",
    "    \"\"\"\n",
    "    Calculate model score by metric given in metric_name\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Set up evaluator and compute score\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"churn\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=metric_name)\n",
    "    score = evaluator.evaluate(predictions)\n",
    "    print(\"Score = \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.37 s, sys: 1.16 s, total: 5.53 s\n",
      "Wall time: 15min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune model\n",
    "rf = RandomForestClassifier(labelCol=\"churn\", featuresCol=\"features\", \n",
    "                            seed = 10)\n",
    "rf_pipeline = Pipeline(stages=[feature_pipeline, rf])\n",
    "\n",
    "# set parameters grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [5, 7])\n",
    "            .addGrid(rf.numTrees, [20, 30])\n",
    "            .build()\n",
    "            )\n",
    "\n",
    "# choose evaluater\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"churn\", \n",
    "                                               predictionCol=\"prediction\", \n",
    "                                               metricName=\"f1\")\n",
    "\n",
    "# define cross-validator\n",
    "crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,\n",
    "                          seed=10)\n",
    "\n",
    "# run cross-validation\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_baefa1457e50', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7,\n",
       " Param(parent='RandomForestClassifier_baefa1457e50', name='numTrees', doc='Number of trees to train (>= 1).'): 20}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check best combination of parameters\n",
    "cvModel.getEstimatorParamMaps()[ np.argmax(cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score =  0.9691768430574401\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "score_the_model(test_data, cvModel, metric_name='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.write().overwrite().save(\"./saved_models/rf_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.66 s, sys: 1.88 s, total: 11.5 s\n",
      "Wall time: 1h 12min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune model\n",
    "gbt = GBTClassifier(labelCol=\"churn\", featuresCol=\"features\")\n",
    "gbt_pipeline = Pipeline(stages=[feature_pipeline, gbt])\n",
    "\n",
    "# set parameters grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(gbt.maxDepth, [3, 5])\n",
    "            .addGrid(gbt.maxIter, [5, 10])\n",
    "            .build()\n",
    "            )\n",
    "\n",
    "# choose evaluater\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"churn\", \n",
    "                                               predictionCol=\"prediction\", \n",
    "                                               metricName=\"f1\")\n",
    "\n",
    "# define cross-validator\n",
    "crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,\n",
    "                          seed=10)\n",
    "\n",
    "# run cross-validation\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_3e458a74b3a7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3,\n",
       " Param(parent='GBTClassifier_3e458a74b3a7', name='maxIter', doc='max number of iterations (>= 0).'): 5}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check best combination of parameters\n",
    "cvModel.getEstimatorParamMaps()[ np.argmax(cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score =  1.0\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "score_the_model(test_data, cvModel, metric_name='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.write().overwrite().save(\"./saved_models/gbt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
