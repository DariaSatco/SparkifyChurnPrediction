{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Sample dataset notebook\n",
    "This notebook contains steps of exploration, processing and modeling with a tiny subset (128MB) of the full dataset available (12GB). Full dataset is treated separately in the notebook on AWS platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, CountVectorizerModel\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from typing import Dict\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimestamp coefficient\n",
    "TS_COEF = 1000*60*60*24\n",
    "\n",
    "# today date\n",
    "TODAY = str(datetime.today().date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this notebook we load the mini-dataset from locally stored file `mini_sparkify_event_data.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Since we are looking at a small subset, it's quite convenient to perform EDA using pandas.\n",
    "Our analysis consists of 3 steps:\n",
    "* Explore Data\n",
    "* Define Churn\n",
    "* Explore churned vs stayed users\n",
    "\n",
    "#### Explore Data\n",
    "I used [sweetviz](https://pypi.org/project/sweetviz/) package to visualize data and make first observations. At this stage we identify the structure of each column, check the nulls and ranges/lists of column values.\n",
    "\n",
    "#### Define Churn\n",
    "I create a column `churn` to use as the label for your model. I used the `Cancellation Confirmation` events to define the churn, which happen for both paid and free users.\n",
    "\n",
    "#### Explore churned vs stayed users\n",
    "Once we've defined churn, we run exploratory data analysis by comparing users who stayed vs users who churned. This is important for the next stage of feature engineering. Looking at major differences, we define the logic for user-level features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA observations\n",
    "I convert Spark dataframe into pandas dataframe to run EDA with more flexibility. Using `sweetviz` I look at the major properties of each column. \n",
    "\n",
    "Here are the **first observations**:\n",
    "1. There are 225 registered users in the dataset and 2354 sessions during 63 days. 97% of records cover the events for these users and only 3% include the data about the guests.\n",
    "2. For guest users (`auth='Guest'`) we don't have neiver songs data or user demographics data, nor *userId*. Their page visits are limited to: Home, Help, Register, About, Submit Registration, Error. We exclude guest users from model dataset.\n",
    "3. There are 3% of records with `auth='Logged Out'`, which include Home, Login, About, Help and Error events. There is no *userId* data for these events, so we exclude them from modelling dataset.\n",
    "3. 80% of records describe NextSong event and include artist and song data. 20% of events cover all over possible actions.\n",
    "4. We have 52 cancellation events, which are described by `auth='Cancelled'` and `page='Cancellation Confirmation'`. There are 52 unique userId, who cancelled subscription. So this event is unique per user.]\n",
    "\n",
    "We **define Churn** as a fact of cancellation of subscription from existing user. The fact of cancellation is translated through 2 columns: `auth='Cancelled'` and `page='Cancellation Confirmation'`, which are uniquely defined, so we can use any of 2 to define the target. Let's use `page='Cancellation Confirmation'` as our target. \n",
    "\n",
    "Before moving to the Feature engineering step, let's **compare** behaviour of **churned users VS stayed users**. Here are some observations:\n",
    "1. Among those who churn there are more males (57% M / 43% F), and vice versa, there are more women among those users, who stay subscrbed (42% M / 58% F).\n",
    "2. Churned users usually have smaller number of items in session (churn median = 66 VS other median = 71).\n",
    "3. Among churned users there are more free users (28% in churn VS 19% in other)\n",
    "4. From page events statistics we see that among churned there are less *Thumbs Up*, more *Thumbs Down*, almost two times higher frequency of *Roll Advert*.\n",
    "5. Churned users have smaller lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>277.89016</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Iron Frenzy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>Long</td>\n",
       "      <td>236.09424</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352180000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist       auth firstName gender  itemInSession lastName  \\\n",
       "0    Martha Tilston  Logged In     Colin      M             50  Freeman   \n",
       "1  Five Iron Frenzy  Logged In     Micah      M             79     Long   \n",
       "\n",
       "      length level                        location method      page  \\\n",
       "0  277.89016  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "1  236.09424  free  Boston-Cambridge-Newton, MA-NH    PUT  NextSong   \n",
       "\n",
       "   registration  sessionId       song  status             ts  \\\n",
       "0  1.538173e+12         29  Rockpools     200  1538352117000   \n",
       "1  1.538332e+12          8     Canada     200  1538352180000   \n",
       "\n",
       "                                           userAgent userId  \n",
       "0  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "1  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_data = df.toPandas()\n",
    "pandas_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286500, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of records and columns in the subset\n",
    "pandas_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date is 2018-10-01\n",
      "Last date is 2018-12-03\n",
      "Total number of days: 63\n"
     ]
    }
   ],
   "source": [
    "# explore time range of data provided\n",
    "print('Earliest date is', pd.to_datetime(pandas_data['ts'], unit='ms').dt.date.min())\n",
    "print('Last date is', pd.to_datetime(pandas_data['ts'], unit='ms').dt.date.max())\n",
    "print('Total number of days:', (pandas_data['ts'].max() - pandas_data['ts'].min())//(TS_COEF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1274eb628d443e39a593a2fd4c67b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "                                             |          | [  0%]   00:00 -> (? left)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report ./EDA_reports/sample_data_overview.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "# generate sweetviz report\n",
    "analysis = sv.analyze([pandas_data, 'sample_data'])\n",
    "analysis.show_html('./EDA_reports/sample_data_overview.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97 entries, 97633 to 199445\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   artist         0 non-null      object \n",
      " 1   auth           97 non-null     object \n",
      " 2   firstName      0 non-null      object \n",
      " 3   gender         0 non-null      object \n",
      " 4   itemInSession  97 non-null     int64  \n",
      " 5   lastName       0 non-null      object \n",
      " 6   length         0 non-null      float64\n",
      " 7   level          97 non-null     object \n",
      " 8   location       0 non-null      object \n",
      " 9   method         97 non-null     object \n",
      " 10  page           97 non-null     object \n",
      " 11  registration   0 non-null      float64\n",
      " 12  sessionId      97 non-null     int64  \n",
      " 13  song           0 non-null      object \n",
      " 14  status         97 non-null     int64  \n",
      " 15  ts             97 non-null     int64  \n",
      " 16  userAgent      0 non-null      object \n",
      " 17  userId         97 non-null     object \n",
      "dtypes: float64(2), int64(4), object(12)\n",
      "memory usage: 14.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# explore data structure for guest visitors\n",
    "pandas_data[pandas_data['auth']=='Guest'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Home                   36\n",
       "Help                   23\n",
       "Register               18\n",
       "About                  14\n",
       "Submit Registration     5\n",
       "Error                   1\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check what actions are available for guest users\n",
    "pandas_data[pandas_data['auth']=='Guest']['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Home                         14457\n",
       "Thumbs Up                    12551\n",
       "Add to Playlist               6526\n",
       "Add Friend                    4277\n",
       "Roll Advert                   3933\n",
       "Login                         3241\n",
       "Logout                        3226\n",
       "Thumbs Down                   2546\n",
       "Downgrade                     2055\n",
       "Help                          1726\n",
       "Settings                      1514\n",
       "About                          924\n",
       "Upgrade                        499\n",
       "Save Settings                  310\n",
       "Error                          258\n",
       "Submit Upgrade                 159\n",
       "Submit Downgrade                63\n",
       "Cancellation Confirmation       52\n",
       "Cancel                          52\n",
       "Register                        18\n",
       "Submit Registration              5\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the events associated with empty song data\n",
    "pandas_data[pandas_data['song'].isnull()]['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextSong    228108\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the events associated with full song data\n",
    "pandas_data[~pandas_data['song'].isnull()]['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cancellation Confirmation    52\n",
       "Name: page, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check page events associated with Cancelled status\n",
    "pandas_data[pandas_data['auth']=='Cancelled']['page'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cancelled</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guest</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logged In</th>\n",
       "      <td>225</td>\n",
       "      <td>278102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logged Out</th>\n",
       "      <td>1</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userId      ts\n",
       "auth                      \n",
       "Cancelled       52      52\n",
       "Guest            1      97\n",
       "Logged In      225  278102\n",
       "Logged Out       1    8249"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check unique user and total count per different values of auth status\n",
    "pandas_data.groupby('auth').agg({'userId': pd.Series.nunique,\n",
    "                                 'ts': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4296b2dc228e4145998bf7628d9e1db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>TqdmHBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "                                             |          | [  0%]   00:00 -> (? left)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report ./EDA_reports/Churn vs stayed.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "# compare 2 subsets: churned users VS stayed users\n",
    "known_users_df = pandas_data[pandas_data['userId']!=''].copy()\n",
    "\n",
    "# add lifetime column\n",
    "known_users_df['max_ts'] = known_users_df.groupby('userId')['ts'].transform('max')\n",
    "known_users_df['lifetime'] = (known_users_df['max_ts']-known_users_df['registration'])/TS_COEF\n",
    "\n",
    "# list of churned users\n",
    "churned_uid_list = pandas_data[pandas_data['page']=='Cancellation Confirmation']['userId'].to_list()\n",
    "\n",
    "report = sv.compare_intra(known_users_df, known_users_df['userId'].isin(churned_uid_list), [\"Churn\", \"Stayed\"])\n",
    "report.show_html('./EDA_reports/Churn vs stayed.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date of cancellation is 2018-10-01\n",
      "Last date of cancellation is 2018-11-29\n",
      "Total number of days between first and last: 58\n"
     ]
    }
   ],
   "source": [
    "# explore time range of cancellations\n",
    "cancel_data = pandas_data[pandas_data['page']=='Cancellation Confirmation']\n",
    "\n",
    "print('Earliest date of cancellation is', pd.to_datetime(cancel_data['ts'], unit='ms').dt.date.min())\n",
    "print('Last date of cancellation is', pd.to_datetime(cancel_data['ts'], unit='ms').dt.date.max())\n",
    "print('Total number of days between first and last:', (cancel_data['ts'].max() - cancel_data['ts'].min())//(TS_COEF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the modelling dataset\n",
    "1. Exclude records with empty *userId*.\n",
    "2. Add label: 1 = Churn, 0 = Not churn. Condition: `page='Cancellation Confirmation'`\n",
    "3. Remove records of `page='Cancellation Confirmation'`.\n",
    "4. Sort dataframe by `userId` and `ts`\n",
    "5. Aggregate features at user level:\n",
    "    * create list of songs\n",
    "    * create list of artists\n",
    "    * list of page events (Cancellation Confirmation and Cancel events preliminary filtered out to remove the leak)\n",
    "    * session frequency\n",
    "    * average number of songs per session\n",
    "    * binary feature: Male gender = 1/0\n",
    "    * binary feature: paid acoount = 1/0\n",
    "    * lifetime (days): time difference between last activity and registration date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Aggregate user-level properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate App data at user level and collects\n",
    "    dataframe for further feature engineering steps\n",
    "    =================\n",
    "    Args:\n",
    "        df (pyspark Dataframe) : data extraction from Sparkify\n",
    "        \n",
    "    Return:\n",
    "        preprocessed pyspark dataframe\n",
    "    \"\"\"\n",
    "    w = Window.partitionBy(df.userId).orderBy(df.ts)\n",
    "    w_uid = Window.partitionBy(df.userId)\n",
    "\n",
    "    preprocessed_df = (df\n",
    "                       .filter(F.col('userId')!='') #filter out guests\n",
    "                       .withColumn('cancelled', (F.col('page')=='Cancellation Confirmation').cast(IntegerType())) \n",
    "                       .withColumn('churn', F.max('cancelled').over(w_uid)) # define churn label\n",
    "                       .withColumn('current_level', F.last('level').over(w)) # sort levels of subscription by date\n",
    "                       .withColumn('last_userAgent', F.last('userAgent').over(w)) # sort agents by date\n",
    "                       .filter(~F.col('page').isin(['Cancellation Confirmation',\n",
    "                                                    'Cancel'])) #remove cancellation page events from dataset\n",
    "                       .groupby('userId') # aggregate features at user level\n",
    "                       .agg(F.collect_list('artist').alias('artist_list'), # combine into list all artist\n",
    "                            F.collect_list('song').alias('song_list'), # combine into list all songs\n",
    "                            F.collect_list('page').alias('page_list'), # combine into list all page events\n",
    "                            F.countDistinct('sessionId').alias('session_count'), # calculate total number of sessions\n",
    "                            F.count('song').alias('song_count'), # calculate total number of songs\n",
    "                            F.first('gender').alias('gender'), # gender data\n",
    "                            F.last('current_level').alias('current_level'), # take last level value\n",
    "                            F.max('churn').alias('churn'), \n",
    "                            F.min('ts').alias('min_ts'), # start timestamp \n",
    "                            F.max('ts').alias('max_ts'), # end timestamp\n",
    "                            F.last('last_userAgent').alias('last_userAgent'), # recent agent\n",
    "                            F.min('registration').alias('registration') # registration date\n",
    "                           )\n",
    "                       # frequency of sessions\n",
    "                       .withColumn('session_freq', F.col('session_count')/((F.col('max_ts')-F.col('min_ts'))/TS_COEF))\n",
    "                       # avg number of songs per session\n",
    "                       .withColumn('song_per_session', F.col('song_count')/F.col('session_count'))\n",
    "                       # binary feature: Male = 1/0\n",
    "                       .withColumn('gender_Male', (F.col('gender')=='M').cast(IntegerType()))\n",
    "                       # binary feature: paid = 1/0\n",
    "                       .withColumn('is_paid', (F.col('current_level')=='paid').cast(IntegerType()))\n",
    "                       # lifetime\n",
    "                       .withColumn('lifetime', (F.col('max_ts')-F.col('registration'))/TS_COEF)\n",
    "                       # extract device/OS pointers from agent\n",
    "                       .withColumn('agent_Windows', F.col('last_userAgent').contains('Windows').cast(IntegerType()))\n",
    "                       .withColumn('agent_Mac', F.col('last_userAgent').contains('Mac').cast(IntegerType()))\n",
    "                       .withColumn('agent_iPhone', F.col('last_userAgent').contains('iPhone').cast(IntegerType()))\n",
    "                       .withColumn('agent_iPad', F.col('last_userAgent').contains('iPad').cast(IntegerType()))\n",
    "                       .withColumn('agent_Linux', F.col('last_userAgent').contains('Linix').cast(IntegerType()))\n",
    "                      ).cache()\n",
    "    \n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = preprocess_data(df)\n",
    "preprocessed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  count\n",
       "0      1     52\n",
       "1      0    173"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.groupby('churn').count().toPandas()\n",
    "# churn <--> 23%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Prepare transformers to collect feature vector\n",
    "\n",
    "Used features:\n",
    "* Apply TF-IDF to artist list, song list and page list. We limit vocabSize to 100 elements\n",
    "* Beside TF-IDF generated features keep session frequency, avg number of songs per session, lifetime, gender, paid, agent based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_transformer(list_name: str,\n",
    "                       vocabSize: int=100):\n",
    "    \"\"\"\n",
    "    Combines TF and IDF pyspark transformers\n",
    "    ------------\n",
    "    \n",
    "    Args:\n",
    "        list_name (string) : prefix of the feature with work list in the format\n",
    "            prefix_list\n",
    "        vocabSize (int)    : number of top-output words to keep\n",
    "    \n",
    "    Returns:\n",
    "        tf transformer, idf transformer\n",
    "    \"\"\"\n",
    "    tf = CountVectorizer(inputCol=f\"{list_name}_list\", outputCol=f\"TF_{list_name}\", vocabSize=vocabSize)\n",
    "    tf_idf = IDF(inputCol=f\"TF_{list_name}\", outputCol=f\"TFIDF_{list_name}\")\n",
    "    return tf, tf_idf\n",
    "\n",
    "\n",
    "artist_tf, artist_tf_idf = tf_idf_transformer('artist')\n",
    "song_tf, song_tf_idf = tf_idf_transformer('song')\n",
    "page_tf, page_tf_idf = tf_idf_transformer('page')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"TFIDF_artist\", \"TFIDF_song\", \"TFIDF_page\",\n",
    "                                       \"session_freq\", \"song_per_session\", \n",
    "                                       \"lifetime\", \"gender_Male\", \n",
    "                                       \"is_paid\", \"agent_Windows\",\n",
    "                                       \"agent_Mac\", \"agent_iPhone\", \"agent_iPad\", \n",
    "                                       \"agent_Linux\"], \n",
    "                            outputCol=\"features\", \n",
    "                            handleInvalid=\"skip\")\n",
    "\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[artist_tf, artist_tf_idf, \n",
    "                                   song_tf, song_tf_idf,\n",
    "                                   page_tf, page_tf_idf,\n",
    "                                   assembler\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = feature_pipeline.fit(preprocessed_df)\n",
    "test_df = test.transform(preprocessed_df)\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Extract feature names and vocabularies from feature transformers to use it later for feature importance analyzis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vocabularies for future explanations\n",
    "stages = test.stages\n",
    "vectorizers = [s for s in stages if isinstance(s, CountVectorizerModel)]\n",
    "vocab_dict = {}\n",
    "vocab_dict['artist'] = vectorizers[0].vocabulary\n",
    "vocab_dict['song'] = vectorizers[1].vocabulary\n",
    "vocab_dict['page'] = vectorizers[2].vocabulary\n",
    "\n",
    "\n",
    "# extract feature names after vector assembler\n",
    "attrs = sorted(\n",
    "    (attr[\"idx\"], attr[\"name\"]) for attr in (chain(*test_df\n",
    "        .schema[\"features\"]\n",
    "        .metadata[\"ml_attr\"][\"attrs\"].values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "We split the full dataset into train (70%) and test (30%). During cross-validation process train data is additionally split into train and validation subsets. Test data is used only to check the model (nexer seen during training).\n",
    "\n",
    "We try 2 models:\n",
    "* Random Forest Classifier\n",
    "* Gradient Boosted Tree Classifier\n",
    "\n",
    "Note: since we use tree-based models, we don't don't need to scale numerical features.\n",
    "Our problem is imbalanced: 23% of positive cases (churn) and 67% of negative (stayed). Thus, we use F1-score to tune hyperparameters and check final quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = preprocessed_df.randomSplit([0.7, 0.3], seed=10)\n",
    "\n",
    "# cache dataframes\n",
    "train_data = train_data.cache()\n",
    "test_data = test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_the_model(test_data: pyspark.sql.DataFrame, \n",
    "                    model: Pipeline, \n",
    "                    metric_name: str='accuracy'):\n",
    "    \"\"\"\n",
    "    Calculate model score by metric given in metric_name\n",
    "    --------------------\n",
    "    Args:\n",
    "        test_data (pyspark DataFrame): dataframe with test samples\n",
    "        model (Pipeline)             : pretrained model\n",
    "        metric_name (string)         : one of the MulticlassClassificationEvaluator metrics\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Set up evaluator and compute score\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"churn\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=metric_name)\n",
    "    score = evaluator.evaluate(predictions)\n",
    "    print(\"Score = \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(x: str,\n",
    "           vocab_dict: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Rename raw attribute names according to vocabularies\n",
    "    ----------------\n",
    "    Args:\n",
    "        x (string) : original name\n",
    "        vocab_dict (Dict) : dictionary containing all vocabularies\n",
    "    Return:\n",
    "        string : new name\n",
    "    \"\"\"\n",
    "    if 'TFIDF' in x:\n",
    "        components = x.split('_')\n",
    "        new_components = components[:-1]\n",
    "        new_components.append(vocab_dict[components[1]][int(components[-1])])\n",
    "        new_x = '_'.join(new_components)\n",
    "    else:\n",
    "        new_x = x\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 s, sys: 738 ms, total: 4.07 s\n",
      "Wall time: 13min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune model\n",
    "rf = RandomForestClassifier(labelCol=\"churn\", featuresCol=\"features\", \n",
    "                            seed = 10)\n",
    "rf_pipeline = Pipeline(stages=[feature_pipeline, rf])\n",
    "\n",
    "# set parameters grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [5, 7])\n",
    "            .addGrid(rf.numTrees, [20, 30])\n",
    "            .build()\n",
    "            )\n",
    "\n",
    "# choose evaluater\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"churn\", \n",
    "                                               predictionCol=\"prediction\", \n",
    "                                               metricName=\"f1\")\n",
    "\n",
    "# define cross-validator\n",
    "crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,\n",
    "                          seed=10)\n",
    "\n",
    "# run cross-validation\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_6a30cb8053ca', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='RandomForestClassifier_6a30cb8053ca', name='numTrees', doc='Number of trees to train (>= 1).'): 20}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check best combination of parameters\n",
    "cvModel.getEstimatorParamMaps()[ np.argmax(cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score =  0.7254676842034173\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "score_the_model(test_data, cvModel, metric_name='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "cvModel.bestModel.write().overwrite().save(\"./saved_models/rf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "persistedModel = PipelineModel.load(\"./saved_models/rf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name_raw</th>\n",
       "      <th>importance</th>\n",
       "      <th>feature_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>session_freq</td>\n",
       "      <td>0.123965</td>\n",
       "      <td>session_freq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>lifetime</td>\n",
       "      <td>0.060926</td>\n",
       "      <td>lifetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>TFIDF_page_6</td>\n",
       "      <td>0.031007</td>\n",
       "      <td>TFIDF_page_Logout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TFIDF_artist_56</td>\n",
       "      <td>0.022657</td>\n",
       "      <td>TFIDF_artist_Shakira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>TFIDF_page_14</td>\n",
       "      <td>0.018132</td>\n",
       "      <td>TFIDF_page_Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>TFIDF_song_10</td>\n",
       "      <td>0.018080</td>\n",
       "      <td>TFIDF_song_Ain't Misbehavin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF_artist_18</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>TFIDF_artist_Harmonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF_artist_16</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>TFIDF_artist_Linkin Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF_artist_13</td>\n",
       "      <td>0.016618</td>\n",
       "      <td>TFIDF_artist_Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>TFIDF_song_7</td>\n",
       "      <td>0.015998</td>\n",
       "      <td>TFIDF_song_Use Somebody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TFIDF_artist_40</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>TFIDF_artist_Rammstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TFIDF_artist_73</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>TFIDF_artist_Charttraxx Karaoke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TFIDF_artist_63</td>\n",
       "      <td>0.013713</td>\n",
       "      <td>TFIDF_artist_Beirut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TFIDF_artist_36</td>\n",
       "      <td>0.013179</td>\n",
       "      <td>TFIDF_artist_Weezer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TFIDF_artist_50</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>TFIDF_artist_Amy Winehouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>TFIDF_song_14</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>TFIDF_song_Somebody To Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>TFIDF_page_3</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>TFIDF_page_Add to Playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>TFIDF_artist_81</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>TFIDF_artist_LCD Soundsystem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>TFIDF_page_1</td>\n",
       "      <td>0.011389</td>\n",
       "      <td>TFIDF_page_Thumbs Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>TFIDF_song_53</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>TFIDF_song_Fix You</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_name_raw  importance                     feature_name\n",
       "126     session_freq    0.123965                     session_freq\n",
       "128         lifetime    0.060926                         lifetime\n",
       "117     TFIDF_page_6    0.031007                TFIDF_page_Logout\n",
       "40   TFIDF_artist_56    0.022657             TFIDF_artist_Shakira\n",
       "124    TFIDF_page_14    0.018132                 TFIDF_page_Error\n",
       "71     TFIDF_song_10    0.018080      TFIDF_song_Ain't Misbehavin\n",
       "14   TFIDF_artist_18    0.018072            TFIDF_artist_Harmonia\n",
       "12   TFIDF_artist_16    0.017009         TFIDF_artist_Linkin Park\n",
       "9    TFIDF_artist_13    0.016618        TFIDF_artist_Taylor Swift\n",
       "69      TFIDF_song_7    0.015998          TFIDF_song_Use Somebody\n",
       "27   TFIDF_artist_40    0.015682           TFIDF_artist_Rammstein\n",
       "48   TFIDF_artist_73    0.015665  TFIDF_artist_Charttraxx Karaoke\n",
       "45   TFIDF_artist_63    0.013713              TFIDF_artist_Beirut\n",
       "25   TFIDF_artist_36    0.013179              TFIDF_artist_Weezer\n",
       "37   TFIDF_artist_50    0.013017       TFIDF_artist_Amy Winehouse\n",
       "73     TFIDF_song_14    0.012061      TFIDF_song_Somebody To Love\n",
       "114     TFIDF_page_3    0.012038       TFIDF_page_Add to Playlist\n",
       "51   TFIDF_artist_81    0.011756     TFIDF_artist_LCD Soundsystem\n",
       "113     TFIDF_page_1    0.011389             TFIDF_page_Thumbs Up\n",
       "91     TFIDF_song_53    0.011006               TFIDF_song_Fix You"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_tab = pd.DataFrame([(name, persistedModel.stages[-1].featureImportances[idx])\n",
    "                                       for idx, name in attrs\n",
    "                                       if persistedModel.stages[-1].featureImportances[idx]],\n",
    "                                      columns=['feature_name_raw', 'importance'])\n",
    "feature_importance_tab['feature_name'] = feature_importance_tab['feature_name_raw'].apply(lambda x: rename(x, vocab_dict))\n",
    "feature_importance_tab.sort_values(by='importance', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.82 s, sys: 939 ms, total: 5.76 s\n",
      "Wall time: 36min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune model\n",
    "gbt = GBTClassifier(labelCol=\"churn\", featuresCol=\"features\")\n",
    "gbt_pipeline = Pipeline(stages=[feature_pipeline, gbt])\n",
    "\n",
    "# set parameters grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(gbt.maxDepth, [3, 5])\n",
    "            .addGrid(gbt.maxIter, [5, 10])\n",
    "            .build()\n",
    "            )\n",
    "\n",
    "# choose evaluater\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"churn\", \n",
    "                                               predictionCol=\"prediction\", \n",
    "                                               metricName=\"f1\")\n",
    "\n",
    "# define cross-validator\n",
    "crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,\n",
    "                          seed=10)\n",
    "\n",
    "# run cross-validation\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_e80ef7dbf8d5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 3,\n",
       " Param(parent='GBTClassifier_e80ef7dbf8d5', name='maxIter', doc='max number of iterations (>= 0).'): 5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check best combination of parameters\n",
    "cvModel.getEstimatorParamMaps()[ np.argmax(cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score =  0.7313432835820896\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "score_the_model(test_data, cvModel, metric_name='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT model saved to ./saved_models/gbt_model\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "cvModel.bestModel.write().overwrite().save(\"./saved_models/gbt_model\")\n",
    "print('GBT model saved to ./saved_models/gbt_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "persistedModel = PipelineModel.load(\"./saved_models/gbt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name_raw</th>\n",
       "      <th>importance</th>\n",
       "      <th>feature_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF_song_5</td>\n",
       "      <td>0.163089</td>\n",
       "      <td>TFIDF_song_Dog Days Are Over (Radio Edit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lifetime</td>\n",
       "      <td>0.156076</td>\n",
       "      <td>lifetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>session_freq</td>\n",
       "      <td>0.112537</td>\n",
       "      <td>session_freq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TFIDF_artist_4</td>\n",
       "      <td>0.099690</td>\n",
       "      <td>TFIDF_artist_BjÃÂ¶rk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF_song_63</td>\n",
       "      <td>0.094081</td>\n",
       "      <td>TFIDF_song_Tighten Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF_song_59</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>TFIDF_song_I Gotta Feeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF_artist_56</td>\n",
       "      <td>0.054867</td>\n",
       "      <td>TFIDF_artist_Shakira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF_song_58</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>TFIDF_song_Master Of Puppets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF_artist_88</td>\n",
       "      <td>0.035837</td>\n",
       "      <td>TFIDF_artist_The Notorious B.I.G.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF_artist_0</td>\n",
       "      <td>0.032679</td>\n",
       "      <td>TFIDF_artist_Kings Of Leon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TFIDF_page_5</td>\n",
       "      <td>0.026775</td>\n",
       "      <td>TFIDF_page_Roll Advert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF_song_81</td>\n",
       "      <td>0.023812</td>\n",
       "      <td>TFIDF_song_Electric Feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF_page_1</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>TFIDF_page_Thumbs Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TFIDF_artist_58</td>\n",
       "      <td>0.015289</td>\n",
       "      <td>TFIDF_artist_Snow Patrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF_artist_1</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>TFIDF_artist_Coldplay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TFIDF_page_4</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>TFIDF_page_Add Friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TFIDF_page_2</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>TFIDF_page_Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF_song_6</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>TFIDF_song_Secrets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF_song_96</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>TFIDF_song_Forever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF_song_14</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>TFIDF_song_Somebody To Love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_name_raw  importance                               feature_name\n",
       "6      TFIDF_song_5    0.163089  TFIDF_song_Dog Days Are Over (Radio Edit)\n",
       "19         lifetime    0.156076                                   lifetime\n",
       "18     session_freq    0.112537                               session_freq\n",
       "2    TFIDF_artist_4    0.099690                      TFIDF_artist_BjÃÂ¶rk\n",
       "11    TFIDF_song_63    0.094081                      TFIDF_song_Tighten Up\n",
       "10    TFIDF_song_59    0.057250                 TFIDF_song_I Gotta Feeling\n",
       "3   TFIDF_artist_56    0.054867                       TFIDF_artist_Shakira\n",
       "9     TFIDF_song_58    0.042169               TFIDF_song_Master Of Puppets\n",
       "5   TFIDF_artist_88    0.035837          TFIDF_artist_The Notorious B.I.G.\n",
       "0    TFIDF_artist_0    0.032679                 TFIDF_artist_Kings Of Leon\n",
       "17     TFIDF_page_5    0.026775                     TFIDF_page_Roll Advert\n",
       "12    TFIDF_song_81    0.023812                   TFIDF_song_Electric Feel\n",
       "14     TFIDF_page_1    0.019760                       TFIDF_page_Thumbs Up\n",
       "4   TFIDF_artist_58    0.015289                   TFIDF_artist_Snow Patrol\n",
       "1    TFIDF_artist_1    0.014732                      TFIDF_artist_Coldplay\n",
       "16     TFIDF_page_4    0.014401                      TFIDF_page_Add Friend\n",
       "15     TFIDF_page_2    0.012419                            TFIDF_page_Home\n",
       "7      TFIDF_song_6    0.010733                         TFIDF_song_Secrets\n",
       "13    TFIDF_song_96    0.007594                         TFIDF_song_Forever\n",
       "8     TFIDF_song_14    0.006211                TFIDF_song_Somebody To Love"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_tab = pd.DataFrame([(name, persistedModel.stages[-1].featureImportances[idx])\n",
    "                                       for idx, name in attrs\n",
    "                                       if persistedModel.stages[-1].featureImportances[idx]],\n",
    "                                      columns=['feature_name_raw', 'importance'])\n",
    "feature_importance_tab['feature_name'] = feature_importance_tab['feature_name_raw'].apply(lambda x: rename(x, vocab_dict))\n",
    "feature_importance_tab.sort_values(by='importance', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
